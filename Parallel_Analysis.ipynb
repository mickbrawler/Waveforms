{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00f4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "\n",
    "def waveforms(N_A, N_g, N_f, t0_tf, T, B, trials, seedn=1, inputfile=\"input\", \n",
    "              phi0=0, A0=1, Af=50, g0=0, gf=2, F0=90, Ff=110, N=10000):\n",
    "\n",
    "    # initalizes the arrays which span parameter space, and their lengths\n",
    "    A_RANGE=np.linspace(A0,Af,N_A)\n",
    "    G_RANGE=np.linspace(g0,gf,N_g)\n",
    "    F_RANGE=np.linspace(F0,Ff,N_f)\n",
    "\n",
    "    # number of parameters available\n",
    "    A_LEN, G_LEN, F_LEN = N_A, N_g, N_f\n",
    "    \n",
    "    waveform_data={}\n",
    "    for j in range(trials):\n",
    "        waveform_data.update({j:[[],[]]})\n",
    "        \n",
    "        # calculates random indice for each parameter (A, f, g)\n",
    "        A_RAN=random.randint(0,A_LEN-1)\n",
    "        G_RAN=random.randint(0,G_LEN-1)\n",
    "        F_RAN=random.randint(0,F_LEN-1)\n",
    "        \n",
    "        # calculates random parameters A, f, g\n",
    "        A, gamma, f = A_RANGE[A_RAN], G_RANGE[G_RAN], F_RANGE[F_RAN]\n",
    "        \n",
    "        dt=T/N # time resolution\n",
    "\n",
    "        t0=(T-t0_tf)*np.random.random(1)[0]  # randomly generate start time\n",
    "        START_INDEX=math.floor(t0/dt)        # find index associated with time\n",
    "\n",
    "        ##NOTE: using 't0' instead of some multiple of dt may cause issues later\n",
    "        \n",
    "        SIG_LEN = (math.floor(t0_tf/dt)+1 if (t0_tf != T) else N) # calculate # of indexes signal takes\n",
    "        INJECTED = np.zeros(N)                 # initalize injected signal, with N size array of zeroes\n",
    "        for i in range(SIG_LEN):\n",
    "            INJECTED[START_INDEX + i]=t0+i*dt       # fill in injected signal with its time stamps\n",
    "\n",
    "        w = 2 * np.pi * f\n",
    "        \n",
    "        # replace timestamps with their displacement values\n",
    "        SR = INJECTED[START_INDEX : START_INDEX+SIG_LEN][:]\n",
    "        INJECTED[START_INDEX : START_INDEX+SIG_LEN] = A*np.sin(w*(SR-t0) + phi0)*np.exp(-gamma*(SR-t0))\n",
    "        \n",
    "        # Purposed for Quadrature Sum\n",
    "        D_i = [] # list of each differently seeded waveform\n",
    "        for n in range(seedn):\n",
    "            np.random.seed(seed = n)\n",
    "            NOISE = np.random.normal(scale=(B/(math.sqrt(3)*2)), size=N)  # Noise!\n",
    "            D_i.append(list(NOISE + INJECTED))  # complete data!\n",
    "        \n",
    "        # gets parameters and data for each trial, stuffs it into dictionary\n",
    "        parameters = [A, f, gamma, t0]\n",
    "        waveform_data[j][0], waveform_data[j][1] = parameters, D_i\n",
    "        \n",
    "    # each trial has list of parameters used and list of data values\n",
    "    with open(\"{}-waveform_data.json\".format(inputfile) , \"w\") as f:\n",
    "        json.dump(waveform_data, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f81f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N_A, N_g, N_f, t0_tf, T, B, trials\n",
    "waveforms(4, 4, 4, 4, 10, 0, 10, N=250, seedn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db8c105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Produces a template given a position in parameter space\n",
    "def template(A, f, gamma, duration, dt):\n",
    "    t = np.arange(0, duration + dt, dt)\n",
    "    w = 2 * np.pi * f\n",
    "    return A * np.sin(w*t)*np.exp(-gamma*t)\n",
    "\n",
    "# Produces a cross corelation function given a data input and a template in parameter space\n",
    "def CrossCorrelation(data, template, dt):\n",
    "    ii = 0\n",
    "    M = []\n",
    "\n",
    "    while len(data[ii:]) >= len(template):\n",
    "        M.append(np.sum((data[ii: len(template) + ii] * template)))\n",
    "        ii+=1\n",
    "\n",
    "    return M\n",
    "\n",
    "# Produces chi square at each \"slide\"\n",
    "def ChiSquare(data, template, dt):\n",
    "    ii = 0\n",
    "    C = []\n",
    "\n",
    "    while len(data[ii:]) >= len(template):\n",
    "        C.append(-1 * np.sum((data[ii: len(template) + ii] - template) ** 2))\n",
    "        ii += 1\n",
    "\n",
    "    return C\n",
    "\n",
    "def modulator(rho_ij, D, dt):\n",
    "\n",
    "    rho_mod_D, RHO_ij = [] , rho_ij[:]\n",
    "\n",
    "    dn , L = math.floor(2*D/dt) , len(RHO_ij)\n",
    "\n",
    "    for i in range(0,L-(L%dn),dn):\n",
    "        rho_mod_D.append(max(RHO_ij[i:i+dn]))\n",
    "\n",
    "    if (L-(L%dn)) != L :\n",
    "        rho_mod_D.append(max(RHO_ij[L-(L%dn):L]))\n",
    "\n",
    "    return rho_mod_D\n",
    "\n",
    "def statudio(trialn, D, N_A, N_g, N_f, t0_tf, T, trials, run1 = True,\n",
    "                  seedn=1, inputfile=\"input\", A0=1, Af=50, g0=0, gf=2,\n",
    "                  F0=90, Ff=110, N_t=10000):\n",
    "\n",
    "    # initalizes the arrays which span parameter space, and their lengths\n",
    "    A_RANGE=np.linspace(A0,Af,N_A)\n",
    "    G_RANGE=np.linspace(g0,gf,N_g)\n",
    "    F_RANGE=np.linspace(F0,Ff,N_f)\n",
    "\n",
    "    A_LEN, G_LEN, F_LEN = N_A, N_g, N_f\n",
    "\n",
    "    # constructs timestep resolution, and saves N and t0/tf internally \n",
    "    N, dt = N_t, T/N_t\n",
    "\n",
    "    # constructs time range to pick injected signal start time from/ corresponding length \n",
    "    t_RANGE=np.linspace(0,T-(t0_tf),int(N_t*(1-((t0_tf)/(T)))))\n",
    "    t_LEN=len(t_RANGE)\n",
    "\n",
    "    # initialize arrays for various data/cross-correlations/chi-squares \n",
    "    noise = []\n",
    "\n",
    "    # constructs all templates which correspond to points in the parameter space\n",
    "    TEMPLATES_AFG=[ template( A, f, g, t0_tf, dt) for A in A_RANGE \n",
    "                   for g in G_RANGE for f in F_RANGE]\n",
    "\n",
    "    AFG_PAIR=[ [A, f, g] for A in A_RANGE \n",
    "                   for g in G_RANGE for f in F_RANGE]\n",
    "\n",
    "    # Reads waveform data file \n",
    "    with open(\"{}-waveform_data.json\".format(inputfile),\"r\") as f: \n",
    "        waveform_data = json.load(f)\n",
    "\n",
    "    waveform_data = waveform_data[str(trialn)] # trialn's parameters and differently seeded data\n",
    "    \n",
    "    output={}\n",
    "\n",
    "    output.update({trialn:[[],[],[],[]]})\n",
    "\n",
    "    # isolates random a-g-f pair / data set \n",
    "    temp_AGFT, data = waveform_data[0], waveform_data[1]\n",
    "\n",
    "    noise.append(data) \n",
    "\n",
    "    output[trialn][0], output[trialn][1] = temp_AGFT, data  # stores random a-g-f pair / data set \n",
    "\n",
    "    Quad_CRS = []\n",
    "    Quad_CHI = []\n",
    "    \n",
    "    # performs base static calculation across parameter space\n",
    "    # Quadrature Sum\n",
    "    for n in range(seedn): # Use seedn as index for data\n",
    "\n",
    "        CRS_COR, CHI_SQR = [[],[]]\n",
    "\n",
    "        for temp in TEMPLATES_AFG:\n",
    "\n",
    "            CC_dh = list(CrossCorrelation(data[n], temp, dt))\n",
    "            CRS_COR.append(CC_dh)\n",
    "\n",
    "            CS_dh = list(ChiSquare(data[n], temp, dt))\n",
    "            CHI_SQR.append(CS_dh)\n",
    "\n",
    "        Quad_CRS.append(CRS_COR) # now a 3d list of seedn statistics, with 2d list statistics per waveform\n",
    "        Quad_CHI.append(CHI_SQR)\n",
    "\n",
    "    CRS_COR = np.sum(np.array(Quad_CRS) ** 2, axis = 0) ** .5 # Quadrature sum of each seed's statistic\n",
    "    CHI_SQR = np.sum(np.array(Quad_CHI) ** 2, axis = 0) ** .5\n",
    "\n",
    "    # stores base statistics to attribute\n",
    "    cross_cor = CRS_COR\n",
    "    chi = CHI_SQR\n",
    "    output[trialn][2], output[trialn][3] = CRS_COR.tolist(), CHI_SQR.tolist() # store quadrature summed based statistics\n",
    "    \n",
    "#calculates test statistic, stores it internally,\n",
    "#and returns a copy of it as a dictionary \n",
    "# trying connect stat and anlysis function\n",
    "    \n",
    "    # counts number of tempates in parameter space\n",
    "    PSPACE_LEN = len(AFG_PAIR)\n",
    "    \n",
    "    # String to equation!\n",
    "    stats = [\"CC_IJ\",\"CS_IJ\",\"CC_IJ/abs((1+CS_IJ))\"]\n",
    "\n",
    "    # initalizes rho statistic dictionary/stat outputs collector\n",
    "    RHO = {}\n",
    "    stat_outputs = []\n",
    "    \n",
    "    for stat in stats: # objective is that the appropriate jsons holds multiple stat results\n",
    "\n",
    "        # indexed to loops through parameter space templates and\n",
    "        # calculates each rho_ij given template j\n",
    "        rho_i = []\n",
    "        for j in range(PSPACE_LEN):\n",
    "            CC_IJ = np.array(cross_cor[j][:])\n",
    "            CS_IJ = np.array(chi[j][:])\n",
    "\n",
    "            # Evaluates string (Exec gave issues... eval is the same concept though)\n",
    "            p = eval(stat)\n",
    "            rho_i.append(list(p))\n",
    "        stat_outputs.append(rho_i) # stat_outputs is 3d list holding a 2d list of a stat's outputs for each template\n",
    "    RHO.update({ trialn : stat_outputs })\n",
    "\n",
    "    if (2*D >= dt):\n",
    "        \n",
    "        # gets the length of linearized template space\n",
    "        TEMP_LEN=len(cross_cor) # number of templates \n",
    "\n",
    "        RHO_MOD={}\n",
    "        MAX_OS={}\n",
    "        MAX_BG_TEMP={}\n",
    "        BG_VALS_IJ, FG_VAL_IJ = [], []\n",
    "        pot_thresholds = {}\n",
    "        \n",
    "        # seperates fg value from bg value\n",
    "        T0_2D = math.floor(output[trialn][0][3]/(2*D))\n",
    "\n",
    "        for j in range(TEMP_LEN):\n",
    "            MAX_BG_TEMP.update({ j : list(np.zeros(len(stats)))})\n",
    "\n",
    "        for stat in range(len(stats)):\n",
    "            \n",
    "            pot_thresholds.update({stat:[]})\n",
    "            \n",
    "            BG_VALS_ij, FG_VAL_ij = [], []\n",
    "            for j in range(TEMP_LEN):\n",
    "\n",
    "                # calculates bg values + fg values\n",
    "                BG_VALS_ij.append(modulator(RHO[trialn][stat][j][:],D,dt))\n",
    "\n",
    "                FG_VAL_ij.append(BG_VALS_ij[j].pop(T0_2D))\n",
    "                    \n",
    "            BG_VALS_IJ.append(BG_VALS_ij) # 3d list\n",
    "            FG_VAL_IJ.append(FG_VAL_ij) # 2d list\n",
    "            \n",
    "            pot_thresholds[stat] += FG_VAL_ij # for each stat key, forground values of trialn are added\n",
    "        RHO_MOD.update({ trialn: [ BG_VALS_IJ, FG_VAL_IJ ] }) # these are the peaks we look for\n",
    "        \n",
    "        stat_dict = {}\n",
    "        for stat in range(len(stats)):\n",
    "            stat_dict.update({stat:[tuple(output[trialn][0][0:3]),max(RHO_MOD[trialn][1][stat])] })\n",
    "        MAX_OS.update({trialn: stat_dict}) # stores t0 and forground peaks for each trial\n",
    "\n",
    "        for stat in range(len(stats)):\n",
    "            for j in range(TEMP_LEN):\n",
    "                new_max=max(RHO_MOD[trialn][0][stat][j]) # max ofsource peak for each template\n",
    "\n",
    "                MAX_BG_TEMP[j][stat] = new_max # every value in MAX_BG_TEMP dictionary changes from 0 to that templates max\n",
    "    else: \n",
    "        print(\"invalid D; it is required that 2*D >= T/N\")\n",
    "\n",
    "    # All jsons below will serve post processing\n",
    "\n",
    "    # output holds trialn's parameters, data, cross-correlation, chi-square\n",
    "    with open(\"output_folder/output_{}.json\".format(trialn), \"w\") as f: \n",
    "        json.dump(output, f, indent=2, sort_keys=True)\n",
    "\n",
    "    # RHO_MOD holds trialn's background and forground values\n",
    "    with open(\"Peaks_folder/Peaks_{}.json\".format(trialn), \"w\") as f:\n",
    "        json.dump(RHO_MOD, f, indent=2, sort_keys=True)\n",
    "\n",
    "    # MAX_OS holds trialn's max onsource peak\n",
    "    with open(\"Max_OS_folder/Max_OS_{}.json\".format(trialn), \"w\") as f:\n",
    "        json.dump(MAX_OS, f, indent=2, sort_keys=True)\n",
    "    \n",
    "    # MAX_BG_TEMP and pot_thresholds were devised wrong. They used to rewrite the same file with\n",
    "    # changed or more value in every run. This won't work when parallelized as some runs may finish\n",
    "    # faster than even the first. They are now made for each trial, but because of their different form\n",
    "    # compared to the simple first three, weirder json combiners must be used on them\n",
    "\n",
    "    with open(\"Max_BG_TEMP_folder/Max_BG_TEMP{}.json\".format(trialn), \"w\") as f: # Merged\n",
    "        json.dump(MAX_BG_TEMP, f, indent=2, sort_keys=True)\n",
    "        \n",
    "    with open(\"thresholds_folder/thresholds{}.json\".format(trialn), \"w\") as f:\n",
    "        json.dump(pot_thresholds, f, indent=2, sort_keys=True)\n",
    "    \n",
    "    # Now only the essentials file is made when run1 is True\n",
    "    # essentials holds all the values that don't change for each trialn run\n",
    "    if run1 == True:\n",
    "        essent = {\"essentials\":[A_LEN,F_LEN,G_LEN,list(F_RANGE),list(A_RANGE),list(G_RANGE),AFG_PAIR,trials]}\n",
    "        with open(\"essentials.json\", \"w\") as f:\n",
    "            json.dump(essent, f, indent=2, sort_keys=True)\n",
    "    \n",
    "    return(MAX_BG_TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c474e2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0.2444605573891365, 55.14750191152992, 0.004430464408788846],\n",
       " 1: [9.270460463768025, 73.54936943627463, 0.24592413694069895],\n",
       " 2: [9.270460463768124, 73.77826555342466, 0.2414136025663782],\n",
       " 3: [0.2444605573892886, 55.15502792236894, 0.0043557647916079145],\n",
       " 4: [0.2283342697424941, 14.41607797688298, 0.01572133249683823],\n",
       " 5: [5.521577987297916, 25.033496375496302, 0.8279103862314042],\n",
       " 6: [5.521577987297962, 23.33621494790189, 1.4106468031564248],\n",
       " 7: [0.22833426974250715, 14.43718674555614, 0.014973051718731784],\n",
       " 8: [0.21677525642441603, 9.721094212214856, 0.02192378107971594],\n",
       " 9: [3.726209597885268, 16.787933055037154, 1.3691086177901375],\n",
       " 10: [3.7262095978852816, 15.573399184035587, 2.787370997212679],\n",
       " 11: [0.21677525642442608, 9.754777591898128, 0.020156182177837774],\n",
       " 12: [0.20564736949852033, 8.109160945612786, 0.02468449544063521],\n",
       " 13: [2.981533894359787, 13.12319899734397, 1.397137224522042],\n",
       " 14: [2.9815338943597727, 12.804930216359569, 2.3774302232618436],\n",
       " 15: [0.20564736949852655, 8.153623425724172, 0.02246622566104276],\n",
       " 16: [4.23731632807837, 15035.222046186145, 0.0002821226190141794],\n",
       " 17: [160.68798137197908, 15515.528930204247, 0.010800556680833658],\n",
       " 18: [160.68798137198084, 15519.501852686908, 0.01066417844911426],\n",
       " 19: [4.237316328080998, 15035.362961685994, 0.00028180460520127514],\n",
       " 20: [3.9577940088698984, 2810.1895451782175, 0.00141565968564782],\n",
       " 21: [95.7073517798305, 2997.008511400116, 0.03192364244994545],\n",
       " 22: [95.70735177983131, 2967.5902897323504, 0.03659686314530707],\n",
       " 23: [3.9577940088701213, 2810.555352541052, 0.0014076884544681335],\n",
       " 24: [3.7574377780232027, 1409.7127386293337, 0.002691060118057427],\n",
       " 25: [64.58763303001129, 1536.252299614043, 0.04205728963516806],\n",
       " 26: [64.58763303001153, 1515.1995458270937, 0.05056355373239466],\n",
       " 27: [3.757437778023383, 1410.2965059163496, 0.0026624013892698564],\n",
       " 28: [3.564554404641013, 934.7688838263584, 0.0038649320069848573],\n",
       " 29: [51.67992083556963, 1036.570706949413, 0.05150149891049575],\n",
       " 30: [51.67992083556939, 1022.142082656965, 0.06220118813984412],\n",
       " 31: [3.56455440464113, 935.5394888541283, 0.003806090877174247],\n",
       " 32: [8.230172098767596, 56693.074368238515, 0.00014525175929153679],\n",
       " 33: [312.1055022801901, 57939.38262626403, 0.00550453751552538],\n",
       " 34: [312.1055022801936, 57947.09957511261, 0.005435743643534895],\n",
       " 35: [8.230172098772705, 56693.34867322742, 0.00014516741600136262],\n",
       " 36: [7.687253747997303, 10574.846760363012, 0.000728939268076418],\n",
       " 37: [185.8931255723631, 10943.113341135215, 0.016985672550891246],\n",
       " 38: [185.8931255723647, 10885.974179227298, 0.018223849941803538],\n",
       " 39: [7.687253747997744, 10575.557266319991, 0.0007268200374120839],\n",
       " 40: [7.298100299621995, 5292.5974652566865, 0.0013860120335416727],\n",
       " 41: [125.44905646213732, 5546.237919447337, 0.022620993832238794],\n",
       " 42: [125.44905646213779, 5505.346945744442, 0.024871462644564468],\n",
       " 43: [7.29810029962235, 5293.731316451034, 0.0013783702823496884],\n",
       " 44: [6.923461439783495, 3501.891680036116, 0.001991391903080144],\n",
       " 45: [100.3783077767795, 3712.6954045171656, 0.027029224759436116],\n",
       " 46: [100.37830777677902, 3683.2134607260823, 0.030305793136201475],\n",
       " 47: [6.923461439783724, 3503.3884276115436, 0.0019756546920520705],\n",
       " 48: [12.223027869456844, 125028.70446806865, 9.779891622487276e-05],\n",
       " 49: [463.5230231884013, 127345.11045761564, 0.003693308150703929],\n",
       " 50: [463.52302318840634, 127356.57143283055, 0.0036473137542940436],\n",
       " 51: [12.223027869464424, 125029.1121625467, 9.776067267358561e-05],\n",
       " 52: [11.416713487124708, 23308.38772353127, 0.0004907295669102935],\n",
       " 53: [276.07889936489573, 23863.3479855808, 0.011568675562881783],\n",
       " 54: [276.07889936489806, 23778.487883432757, 0.012129987104455371],\n",
       " 55: [11.41671348712536, 23309.442928082386, 0.0004897681919793767],\n",
       " 56: [10.838762821220794, 11658.375274094275, 0.0009329522144734717],\n",
       " 57: [186.31047989426335, 12046.74479255492, 0.015466329866477892],\n",
       " 58: [186.31047989426403, 11986.015598936077, 0.016486257450446972],\n",
       " 59: [10.838762821221291, 11660.059209195957, 0.0009294835594929318],\n",
       " 60: [10.282368474926004, 7709.477549574886, 0.001340321105125314],\n",
       " 61: [149.07669471798934, 8040.554327713434, 0.018538293549075404],\n",
       " 62: [149.07669471798866, 7996.019064423714, 0.020023067566260575],\n",
       " 63: [10.282368474926317, 7711.700439697973, 0.0013331735823683788]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (trialn, D, N_A, N_g, N_f, t0_tf, T, trials, run1 = True,\n",
    "# seedn=1, inputfile=\"input\", A0=1, Af=50, g0=0, gf=2, F0=90, Ff=110, N_t=10000):\n",
    "\n",
    "statudio(2, .02, 4, 4, 4, 4, 10, 10, N_t=250, seedn=1, run1=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4312e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# works like a charm, put in your path and\n",
    "# make sure to include the path of the produced json in their merge name\n",
    "\n",
    "# This function will be run on the jsons of output, RHO_MOD, and MAX_OS\n",
    "# It updates all trial dictionaries, and since each dictionary has a different key \n",
    "# (which indicates the trial used), then will be added, till 1 dictionary for all\n",
    "# trials is made\n",
    "\n",
    "def json_stack_keys(jsons_path, merge_path_name):\n",
    "    \n",
    "    # Include last / at end of path!\n",
    "    files = glob.glob(\"{}*.json\".format(jsons_path))\n",
    "\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary = json.load(f)\n",
    "        else:\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary.update(json.load(f))\n",
    "\n",
    "    with open('{}.json'.format(merge_path_name), 'w') as f:\n",
    "        json.dump(C_dictionary, f, indent=2)\n",
    "\n",
    "# This function will be run on the jsons of MAX_BG_TEMP\n",
    "# MAX_BG_TEMP always has the same keys, as they are the templates used per trial\n",
    "# As such we update the max offsource value for each stat of each template of each trial\n",
    "# We therefore have the crosses across all trials\n",
    "\n",
    "def json_update_components(jsons_path, merge_path_name):\n",
    "    \n",
    "    with open(\"Merged_Peaks.json\", \"r\") as f:\n",
    "        RHO_MOD = json.load(f)\n",
    "    \n",
    "    # way of getting the first key of a dictionary, used to get tempn and statn quick and dirty\n",
    "    get_first_key = []\n",
    "    get_first_key += RHO_MOD.keys()\n",
    "    dict_key = get_first_key[0]\n",
    "    \n",
    "    # way of getting tempn and statn from RHO_MOD\n",
    "    tempn = len(RHO_MOD[dict_key][0][0])\n",
    "    statn = len(RHO_MOD[dict_key][0])\n",
    "    \n",
    "    # Include last / at end of path!\n",
    "    files = glob.glob(\"{}*.json\".format(jsons_path))\n",
    "\n",
    "    count = 0\n",
    "    for file in files:\n",
    "#         trialn = \"\" # method of obtaining this file's trialnumber through its name\n",
    "#         for character in file:\n",
    "#             if character.isdigit():\n",
    "#                 trialn += str(character)\n",
    "#         trialn = int(trialn)\n",
    "\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary = json.load(f)\n",
    "        else:\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary_new = json.load(f)\n",
    "                \n",
    "            for stat in range(statn):\n",
    "                for j in range(tempn):\n",
    "                \n",
    "                    if (C_dictionary_new[str(j)][stat] > C_dictionary[str(j)][stat]): # every value in MAX_BG_TEMP dictionary changes from 0 to that templates max\n",
    "                        C_dictionary[str(j)][stat] = C_dictionary_new[str(j)][stat]\n",
    "\n",
    "    with open('{}.json'.format(merge_path_name), 'w') as f:\n",
    "        json.dump(C_dictionary, f, indent=2)\n",
    "\n",
    "# This function will be run on the jsons of pot_threshold\n",
    "# pot_thresholds is a 2d list, where each list is a stat's onsource peaks for a trial\n",
    "# The lists will have each trials values added to them until we havethe onsource peaks for all trials \n",
    "\n",
    "def json_list_append(jsons_path, merge_path_name):\n",
    "\n",
    "    # Include last / at end of path!\n",
    "    files = glob.glob(\"{}*.json\".format(jsons_path))\n",
    "\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary = json.load(f)\n",
    "        else:\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary_new = json.load(f)\n",
    "        \n",
    "            for i in C_dictionary:\n",
    "                C_dictionary[str(i)] += C_dictionary_new[str(i)]\n",
    "\n",
    "    with open('{}.json'.format(merge_path_name), 'w') as f:\n",
    "        json.dump(C_dictionary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcf39003",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stack_keys(\"output_folder/\", \"Merged_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "13b2f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stack_keys(\"Peaks_folder/\", \"Merged_Peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cefe5a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stack_keys(\"Max_OS_folder/\", \"Merged_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "479b08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_update_components(\"MAX_BG_TEMP_folder/\", \"Merged_MAX_BG_TEMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65d9f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list_append(\"thresholds_folder/\", \"Merged_thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c080af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
