{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b99c9b",
   "metadata": {},
   "source": [
    "# Waveform Generator (all trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938cacd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "\n",
    "def waveforms(N_A, N_g, N_f, t0_tf, T, B, trials, seedn=1, inputfile=\"input\", \n",
    "              phi0=0, A0=1, Af=50, g0=0, gf=2, F0=90, Ff=110, N=10000):\n",
    "\n",
    "    # initalizes the arrays which span parameter space, and their lengths\n",
    "    A_RANGE=np.linspace(A0,Af,N_A)\n",
    "    G_RANGE=np.linspace(g0,gf,N_g)\n",
    "    F_RANGE=np.linspace(F0,Ff,N_f)\n",
    "\n",
    "    # number of parameters available\n",
    "    A_LEN, G_LEN, F_LEN = N_A, N_g, N_f\n",
    "    \n",
    "    waveform_data={}\n",
    "    for j in range(trials):\n",
    "        waveform_data.update({j:[[],[]]})\n",
    "        \n",
    "        # calculates random indice for each parameter (A, f, g)\n",
    "        A_RAN=random.randint(0,A_LEN-1)\n",
    "        G_RAN=random.randint(0,G_LEN-1)\n",
    "        F_RAN=random.randint(0,F_LEN-1)\n",
    "        \n",
    "        # calculates random parameters A, f, g\n",
    "        A, gamma, f = A_RANGE[A_RAN], G_RANGE[G_RAN], F_RANGE[F_RAN]\n",
    "        \n",
    "        dt=T/N # time resolution\n",
    "\n",
    "        t0=(T-t0_tf)*np.random.random(1)[0]  # randomly generate start time\n",
    "        START_INDEX=math.floor(t0/dt)        # find index associated with time\n",
    "\n",
    "        ##NOTE: using 't0' instead of some multiple of dt may cause issues later\n",
    "        \n",
    "        SIG_LEN = (math.floor(t0_tf/dt)+1 if (t0_tf != T) else N) # calculate # of indexes signal takes\n",
    "        INJECTED = np.zeros(N)                 # initalize injected signal, with N size array of zeroes\n",
    "        for i in range(SIG_LEN):\n",
    "            INJECTED[START_INDEX + i]=t0+i*dt       # fill in injected signal with its time stamps\n",
    "\n",
    "        w = 2 * np.pi * f\n",
    "        \n",
    "        # replace timestamps with their displacement values\n",
    "        SR = INJECTED[START_INDEX : START_INDEX+SIG_LEN][:]\n",
    "        INJECTED[START_INDEX : START_INDEX+SIG_LEN] = A*np.sin(w*(SR-t0) + phi0)*np.exp(-gamma*(SR-t0))\n",
    "        \n",
    "        # Purposed for Quadrature Sum\n",
    "        D_i = [] # list of each differently seeded waveform\n",
    "        for n in range(seedn):\n",
    "            np.random.seed(seed = n)\n",
    "            NOISE = np.random.normal(scale=(B/(math.sqrt(3)*2)), size=N)  # Noise!\n",
    "            D_i.append(list(NOISE + INJECTED))  # complete data!\n",
    "        \n",
    "        # gets parameters and data for each trial, stuffs it into dictionary\n",
    "        parameters = [A, f, gamma, t0]\n",
    "        waveform_data[j][0], waveform_data[j][1] = parameters, D_i\n",
    "        \n",
    "    # each trial has list of parameters used and list of data values\n",
    "    with open(\"{}-waveform_data.json\".format(inputfile) , \"w\") as f:\n",
    "        json.dump(waveform_data, f, indent=2, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57c1350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N_A, N_g, N_f, t0_tf, T, B, trials\n",
    "waveforms(4, 4, 4, 4, 10, 0, 10, N=250, seedn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17024b09",
   "metadata": {},
   "source": [
    "# Stat Analysis (per trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59782051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Produces a template given a position in parameter space\n",
    "def template(A, f, gamma, duration, dt):\n",
    "    t = np.arange(0, duration + dt, dt)\n",
    "    w = 2 * np.pi * f\n",
    "    return A * np.sin(w*t)*np.exp(-gamma*t)\n",
    "\n",
    "# Produces a cross corelation function given a data input and a template in parameter space\n",
    "def CrossCorrelation(data, template, dt):\n",
    "    ii = 0\n",
    "    M = []\n",
    "\n",
    "    while len(data[ii:]) >= len(template):\n",
    "        M.append(np.sum((data[ii: len(template) + ii] * template)))\n",
    "        ii+=1\n",
    "\n",
    "    return M\n",
    "\n",
    "# Produces chi square at each \"slide\"\n",
    "def ChiSquare(data, template, dt):\n",
    "    ii = 0\n",
    "    C = []\n",
    "\n",
    "    while len(data[ii:]) >= len(template):\n",
    "        C.append(-1 * np.sum((data[ii: len(template) + ii] - template) ** 2))\n",
    "        ii += 1\n",
    "\n",
    "    return C\n",
    "\n",
    "def modulator(rho_ij, D, dt):\n",
    "\n",
    "    rho_mod_D, RHO_ij = [] , rho_ij[:]\n",
    "\n",
    "    dn , L = math.floor(2*D/dt) , len(RHO_ij)\n",
    "\n",
    "    for i in range(0,L-(L%dn),dn):\n",
    "        rho_mod_D.append(max(RHO_ij[i:i+dn]))\n",
    "\n",
    "    if (L-(L%dn)) != L :\n",
    "        rho_mod_D.append(max(RHO_ij[L-(L%dn):L]))\n",
    "\n",
    "    return rho_mod_D\n",
    "\n",
    "def stat_analysis(trialn, D, N_A, N_g, N_f, t0_tf, T, trials, thresholds = False, \n",
    "                  essentials=False, seedn=1, inputfile=\"input\", A0=1, Af=50, g0=0, \n",
    "                  gf=2, F0=90, Ff=110, N_t=10000):\n",
    "\n",
    "    # initalizes the arrays which span parameter space, and their lengths\n",
    "    A_RANGE=np.linspace(A0,Af,N_A)\n",
    "    G_RANGE=np.linspace(g0,gf,N_g)\n",
    "    F_RANGE=np.linspace(F0,Ff,N_f)\n",
    "\n",
    "    A_LEN, G_LEN, F_LEN = N_A, N_g, N_f\n",
    "\n",
    "    # constructs timestep resolution, and saves N and t0/tf internally \n",
    "    N, dt = N_t, T/N_t\n",
    "\n",
    "    # constructs time range to pick injected signal start time from/ corresponding length \n",
    "    t_RANGE=np.linspace(0,T-(t0_tf),int(N_t*(1-((t0_tf)/(T)))))\n",
    "    t_LEN=len(t_RANGE)\n",
    "\n",
    "    # initialize arrays for various data/cross-correlations/chi-squares \n",
    "    noise = []\n",
    "\n",
    "    # constructs all templates which correspond to points in the parameter space\n",
    "    TEMPLATES_AFG=[ template( A, f, g, t0_tf, dt) for A in A_RANGE \n",
    "                   for g in G_RANGE for f in F_RANGE]\n",
    "\n",
    "    AFG_PAIR=[ [A, f, g] for A in A_RANGE \n",
    "                   for g in G_RANGE for f in F_RANGE]\n",
    "\n",
    "    # Reads waveform data file \n",
    "    with open(\"{}-waveform_data.json\".format(inputfile),\"r\") as f: \n",
    "        waveform_data = json.load(f)\n",
    "\n",
    "    waveform_data = waveform_data[str(trialn)] # trialn's parameters and differently seeded data\n",
    "    \n",
    "    output={}\n",
    "\n",
    "    output.update({trialn:[[],[],[],[]]})\n",
    "\n",
    "    # isolates random a-g-f pair / data set \n",
    "    temp_AGFT, data = waveform_data[0], waveform_data[1]\n",
    "\n",
    "    noise.append(data) \n",
    "\n",
    "    output[trialn][0], output[trialn][1] = temp_AGFT, data  # stores random a-g-f pair / data set \n",
    "\n",
    "    Quad_CRS = []\n",
    "    Quad_CHI = []\n",
    "    \n",
    "    # performs base static calculation across parameter space\n",
    "    # Quadrature Sum\n",
    "    for n in range(seedn): # Use seedn as index for data\n",
    "\n",
    "        CRS_COR, CHI_SQR = [[],[]]\n",
    "\n",
    "        for temp in TEMPLATES_AFG:\n",
    "\n",
    "            CC_dh = list(CrossCorrelation(data[n], temp, dt))\n",
    "            CRS_COR.append(CC_dh)\n",
    "\n",
    "            CS_dh = list(ChiSquare(data[n], temp, dt))\n",
    "            CHI_SQR.append(CS_dh)\n",
    "\n",
    "        Quad_CRS.append(CRS_COR) # now a 3d list of seedn statistics, with 2d list statistics per waveform\n",
    "        Quad_CHI.append(CHI_SQR)\n",
    "\n",
    "    CRS_COR = np.sum(np.array(Quad_CRS) ** 2, axis = 0) ** .5 # Quadrature sum of each seed's statistic\n",
    "    CHI_SQR = np.sum(np.array(Quad_CHI) ** 2, axis = 0) ** .5\n",
    "\n",
    "    # stores base statistics to attribute\n",
    "    cross_cor = CRS_COR\n",
    "    chi = CHI_SQR\n",
    "    output[trialn][2], output[trialn][3] = CRS_COR, CHI_SQR # store quadrature summed based statistics\n",
    "    \n",
    "#calculates test statistic, stores it internally,\n",
    "#and returns a copy of it as a dictionary \n",
    "# trying connect stat and anlysis function\n",
    "    \n",
    "    # counts number of tempates in parameter space\n",
    "    PSPACE_LEN = len(AFG_PAIR)\n",
    "    \n",
    "    # String to equation!\n",
    "    stats = [\"CC_IJ\",\"CS_IJ\",\"CC_IJ/abs((1+CS_IJ))\"]\n",
    "\n",
    "    # initalizes rho statistic dictionary/stat outputs collector\n",
    "    RHO = {}\n",
    "    stat_outputs = []\n",
    "    \n",
    "    for stat in stats:\n",
    "\n",
    "        # indexed to loops through parameter space templates and\n",
    "        # calculates each rho_ij given template j\n",
    "        rho_i = []\n",
    "        for j in range(PSPACE_LEN):\n",
    "            CC_IJ = np.array(cross_cor[j][:])\n",
    "            CS_IJ = np.array(chi[j][:])\n",
    "\n",
    "            # Evaluates string (Exec gave issues... eval is the same concept though)\n",
    "            p = eval(stat)\n",
    "            rho_i.append(list(p))\n",
    "        stat_outputs.append(rho_i) # stat_outputs is 3d list holding a 2d list of a stat's outputs for each template\n",
    "    RHO.update({ trialn : stat_outputs })\n",
    "\n",
    "    if (2*D >= dt):\n",
    "        \n",
    "        # gets the length of linearized template space\n",
    "        TEMP_LEN=len(cross_cor) # number of templates \n",
    "\n",
    "        RHO_MOD={}\n",
    "        MAX_OS={}\n",
    "        MAX_BG_TEMP={}\n",
    "        pot_thresholds = [[],[],[]]\n",
    "        BG_VALS_IJ, FG_VAL_IJ = [], []\n",
    "        \n",
    "        # seperates fg value from bg value\n",
    "        T0_2D = math.floor(output[trialn][0][3]/(2*D))\n",
    "\n",
    "        for j in range(TEMP_LEN):\n",
    "            MAX_BG_TEMP.update({ j : list(np.zeros(len(stats)))})\n",
    "\n",
    "        for stat in range(len(stats)):\n",
    "\n",
    "            BG_VALS_ij, FG_VAL_ij = [], []\n",
    "            for j in range(TEMP_LEN):\n",
    "\n",
    "                # calculates bg values + fg values\n",
    "                BG_VALS_ij.append(modulator(RHO[trialn][stat][j][:],D,dt))\n",
    "\n",
    "                FG_VAL_ij.append(BG_VALS_ij[j].pop(T0_2D))\n",
    "                    \n",
    "            BG_VALS_IJ.append(BG_VALS_ij) # 3d list\n",
    "            FG_VAL_IJ.append(FG_VAL_ij) # 2d list\n",
    "            \n",
    "            for j in range(TEMP_LEN):\n",
    "                new_max=max(RHO_MOD[trialn][0][stat][j]) # max ofsource peak for each template\n",
    "\n",
    "                if (MAX_BG_TEMP[j][stat] < new_max): # every value in MAX_BG_TEMP dictionary changes from 0 to that templates max\n",
    "                    MAX_BG_TEMP[j][stat] = new_max\n",
    "                \n",
    "        pot_thresholds += FG_VAL_IJ # 3d, of 2d lists per trial, of 1d lists per stat BAD!\n",
    "        RHO_MOD.update({ trialn: [ BG_VALS_IJ, FG_VAL_IJ ] }) # these are the peaks we look for\n",
    "        MAX_OS.update({ trialn: [tuple(output[trialn][0][0:3]),max(RHO_MOD[trialn][1][stat])] }) # stores t0 and forground peaks for each trial\n",
    "        \n",
    "\n",
    "    else: \n",
    "        print(\"invalid D; it is required that 2*D >= T/N\")\n",
    "\n",
    "    # output holds trialn's parameters, data, cross-correlation, chi-square\n",
    "    with open(\"output_{}.json\".format(trialn), \"w\") as f: \n",
    "        json.dump(output, f, indent=2, sort_keys=True)\n",
    "\n",
    "    # RHO_MOD holds trialn's background and forground values\n",
    "    with open(\"Peaks_{}.json\".format(trialn), \"w\") as f:\n",
    "        json.dump(RHO_MOD, f, indent=2, sort_keys=True)\n",
    "\n",
    "    # MAX_OS holds trialn's max onsource peak\n",
    "    with open(\"Max_OS_{}.json\".format(trialn), \"w\") as f:\n",
    "        json.dump(MAX_OS, f, indent=2, sort_keys=True)\n",
    "\n",
    "    # MAX_BG_TEMP holds trialn's \"new_max\" for each statistic used\n",
    "    with open(\"Max_BG_TEMP{}.json\".format(trialn), \"w\") as f: # Merged\n",
    "        json.dump(MAX_BG_TEMP, f, indent=2, sort_keys=True)\n",
    "\n",
    "# You want bottom two if statements to be False for the first trial run\n",
    "# then change to True for the rest of the instances\n",
    "\n",
    "    # pot_thresholds holds trialn's forground peaks\n",
    "    if thresholds == False: # False means to start the threshold file that will hold the pot_thresholds\n",
    "        with open(\"thresholds.json\", \"w\") as f:\n",
    "            json.dump(pot_thresholds, f, indent=2, sort_keys=True)\n",
    "    else:\n",
    "        with open(\"thresholds.json\", \"r\") as f:\n",
    "            current_thresholds = json.load(f)\n",
    "            \n",
    "        pot_thresholds += current_thresholds\n",
    "        \n",
    "        with open(\"thresholds.json\", \"w\") as f:\n",
    "            json.dump(pot_thresholds, f, indent=2, sort_keys=True)\n",
    "    \n",
    "    # essentials holds all the values that don't change for each trialn run\n",
    "    if essentials == False:\n",
    "        essent = {\"essentials\":[A_LEN,F_LEN,G_LEN,F_RANGE,A_RANGE,G_RANGE,AFG_PAIR,trials]}\n",
    "        with open(\"essentials.json\", \"w\") as f:\n",
    "            json.dump(essent, f, indent=2, sort_keys=True)\n",
    "\n",
    "# Argparse must be added\n",
    "# Bash script to run nohup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97c44230",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3263c28e39fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#                   inputfile=\"input\", A0=1, Af=50, g0=0, gf=2, F0=90, Ff=110, N_t=10000):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mstat_analysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseedn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-c8c654c398c4>\u001b[0m in \u001b[0;36mstat_analysis\u001b[1;34m(trialn, D, N_A, N_g, N_f, t0_tf, T, trials, thresholds, essentials, seedn, inputfile, A0, Af, g0, gf, F0, Ff, N_t)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEMP_LEN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                 \u001b[0mnew_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRHO_MOD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrialn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# max ofsource peak for each template\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mMAX_BG_TEMP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnew_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# every value in MAX_BG_TEMP dictionary changes from 0 to that templates max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# stat_analysis(trialn,D, N_A, N_g, N_f, t0_tf, T, trials, seedn = 1, stat=None, \n",
    "#                   inputfile=\"input\", A0=1, Af=50, g0=0, gf=2, F0=90, Ff=110, N_t=10000):\n",
    "    \n",
    "stat_analysis(1, .02, 4, 4, 4, 4, 10, 10, N_t=250, seedn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1ed33",
   "metadata": {},
   "source": [
    "# Json Combiner (all trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b660d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "def json_combiner(jsons_path, merge_path_name):\n",
    "    \n",
    "    # Include last / at end of path!\n",
    "    files = glob.glob(\"{}*.json\".format(jsons_path))\n",
    "\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary = json.load(f)\n",
    "        else:\n",
    "            with open(file, \"r\") as f:\n",
    "                C_dictionary.update(json.load(f))\n",
    "\n",
    "    with open('{}.json'.format(merge_path_name), 'w') as f:\n",
    "        json.dump(json_list, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6455b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ecc06a7",
   "metadata": {},
   "source": [
    "# Plot Functions (All trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "17c893f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_plotter(T, N, var1=0, var2=0,bg_test=True):\n",
    "# essent = {\"essentials\":[A_LEN,F_LEN,G_LEN,F_RANGE,A_RANGE,G_RANGE,AFG_PAIR,trials]}\n",
    "    with open(\"Merged_essentials.json\", \"r\") as f:\n",
    "        essentials = json.load(f)\n",
    "    A_LEN = essentials[\"essentials\"][0]\n",
    "    F_LEN = essentials[\"essentials\"][1]\n",
    "    G_LEN = essentials[\"essentials\"][2]\n",
    "    F_RANGE = essentials[\"essentials\"][3]\n",
    "    A_RANGE = essentials[\"essentials\"][4]\n",
    "    G_RANGE = essentials[\"essentials\"][5]\n",
    "    AFG_PAIR = essentials[\"essentials\"][6]\n",
    "    trials = essentials[\"essentials\"][7]\n",
    "\n",
    "    with open(\"Merged_output.json\", \"r\") as f:\n",
    "        output = json.load(f)\n",
    "    \n",
    "    with open(\"Merged_Peak.json\", \"r\") as f:\n",
    "        RHO_MOD = json.load(f)\n",
    "\n",
    "    # Calculates the index of the attribute variable\n",
    "    # (A, F, or G) which the heat map should marginalize\n",
    "    # over\n",
    "    if ((var1 <= 2) and (var2 <= 2)):    #checks if range valid\n",
    "        if ((0<= var1) and (0<= var2)):  #checks if range valid\n",
    "            if (var2 < var1):            #properly orders\n",
    "                var1, var2 = var2, var1\n",
    "                index=(1/4)*(var1-1)*(var1-2)*(var2)*(7-3*var2) #maps to needed index\n",
    "\n",
    "            elif (var2==var1):\n",
    "                index=var2  # if the values are the same, set as index\n",
    "\n",
    "            else:\n",
    "                index=(1/4)*(var1-1)*(var1-2)*(var2)*(7-3*var2) #maps to needed index\n",
    "\n",
    "        else:\n",
    "            print(\"improper index inputs: both must be >= 0\")\n",
    "            index=0\n",
    "    else:\n",
    "        print(\"improper index inputs: both must be <= 2\")\n",
    "        index=0\n",
    "\n",
    "    mapping={}   #initalizes mapping for heat map\n",
    "\n",
    "    holder=np.full((A_LEN, F_LEN, G_LEN), 0, dtype=list)\n",
    "\n",
    "    # builds link between keys and np-array \n",
    "    for j in range(F_LEN):\n",
    "        for i in range(A_LEN):\n",
    "            for k in range(G_LEN):\n",
    "                F=F_RANGE[j]\n",
    "                A=A_RANGE[i]\n",
    "                G=G_RANGE[k]\n",
    "\n",
    "                holder[(i,j,k)]=[0]\n",
    "                mapping.update({ (A,F,G) : holder[(i,j,k)]})\n",
    "\n",
    "    #initalizes threshold axis points, counts for each threshold\n",
    "    #and space of parameter space\n",
    "    THRS_AXIS=[]\n",
    "    COUNT_AXIS=[]\n",
    "    PSPACE_LEN=len(AFG_PAIR)\n",
    "    PSACE_PAIRS=[]\n",
    "\n",
    "    #sets current threshold value\n",
    "    # how many templates succeeded per threshold (succ_count_thrhld), how many trials succeeded per threshold (heatcount)\n",
    "    for thrshld in np.linspace(0,T,N):\n",
    "\n",
    "        THRS_AXIS.append(thrshld)\n",
    "        succ_count_thrhld=0\n",
    "\n",
    "        #loops from trial/ parameter space pairs \n",
    "        for i in range(trials):\n",
    "            heat_count=0\n",
    "            for j in range(PSPACE_LEN):\n",
    "\n",
    "                RM_ij=np.array(RHO_MOD[i][0][j])\n",
    "                FG_ij=RHO_MOD[i][1][j]\n",
    "                N_BG_ij=len(RM_ij[ RM_ij > FG_ij])\n",
    "\n",
    "                #tests if the given pair passes the threshold test\n",
    "                if (bg_test==True):\n",
    "                    if ((N_BG_ij==0) and (FG_ij > thrshld)): # (per template) offsource peaks < onsource peak, onsource peak > threshold\n",
    "\n",
    "                        if heat_count==0:\n",
    "                            succ_count_thrhld+=1\n",
    "                            mapping[tuple(output[i][0][:3])][0]+=1 # adds 1 to that paramater combo's value, default is 0 \n",
    "                            heat_count+=1\n",
    "                else:\n",
    "                    if (FG_ij > thrshld): # (per template) onsource peak > threshold\n",
    "\n",
    "                        if heat_count==0:\n",
    "                            succ_count_thrhld+=1\n",
    "                            mapping[tuple(output[i][0][:3])][0]+=1 # adds 1 to that paramater combo's value, default is 0 \n",
    "                            heat_count+=1\n",
    "\n",
    "        COUNT_AXIS.append(succ_count_thrhld)\n",
    "\n",
    "    plt.plot(THRS_AXIS,COUNT_AXIS)\n",
    "    plt.xlabel(\"$Threshold$\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.show()\n",
    "\n",
    "    #redefines things to be normal arrays inside\n",
    "    for j in range(F_LEN):\n",
    "        for i in range(A_LEN):\n",
    "            for k in range(G_LEN):\n",
    "                holder[(i,j,k)]=holder[(i,j,k)][0] # I think [0] is making the list value for each parameter combo just its value\n",
    "\n",
    "    #marginalizes array in direction of index\n",
    "    index=int(index) # index is still weird\n",
    "    w=holder.sum(index)\n",
    "\n",
    "    #builds a copy of the original array, but fixes things to be integers \n",
    "    cop=np.full(w.shape,0) # array in shape of marginalized array of holder values\n",
    "    z=[(i,j) for i in range(w.shape[0]) for j in range(w.shape[1])] \n",
    "    for tup in z: # z holds the index for cop, these indexes are each of two variables in use\n",
    "        cop[tup]=int(w[tup])\n",
    "\n",
    "    label=np.array([\"amplitude\",\"frequency\", \"gamma\"])\n",
    "    x=np.array([0, 1, 2])\n",
    "    x=x[ x != index] # parameter index that aren't index are used\n",
    "\n",
    "    plt.imshow( cop, cmap=plt.cm.hot)\n",
    "    plt.xlabel(label[x[1]])\n",
    "    plt.ylabel(label[x[0]])\n",
    "\n",
    "    #plt.imshow( heat_array, cmap=plt.cm.hot) \n",
    "\n",
    "def ROC_Data(T0, Tf, N):\n",
    "\n",
    "    with open(\"Merged_essentials.json\", \"r\") as f:\n",
    "        essentials = json.load(f)\n",
    "    A_LEN = essentials[\"essentials\"][0]\n",
    "    F_LEN = essentials[\"essentials\"][1]\n",
    "    G_LEN = essentials[\"essentials\"][2]\n",
    "    F_RANGE = essentials[\"essentials\"][3]\n",
    "    A_RANGE = essentials[\"essentials\"][4]\n",
    "    G_RANGE = essentials[\"essentials\"][5]\n",
    "    AFG_PAIR = essentials[\"essentials\"][6]\n",
    "    trials = essentials[\"essentials\"][7]\n",
    "    \n",
    "    with open(\"Merged_Peak.json\", \"r\") as f:\n",
    "        RHO_MOD = json.load(f)\n",
    "\n",
    "    PSPACE_LEN=len(AFG_PAIR)\n",
    "\n",
    "    # Stats per threshold\n",
    "    Detection_Prob = []\n",
    "    New_False_Prob = []\n",
    "\n",
    "    #sets current threshold value\n",
    "    for thrshld in np.linspace(T0,Tf,N):\n",
    "\n",
    "        # Detection/ False Alarm probability counters\n",
    "        Detect_count = 0\n",
    "        False_count = 0\n",
    "\n",
    "        #loops from trial/ parameter space pairs \n",
    "        for i in range(trials):\n",
    "\n",
    "            # Detection Probability\n",
    "            Max_FG_ij = max(RHO_MOD[i][1]) # max of onsources per trial\n",
    "            if Max_FG_ij > thrshld:\n",
    "                Detect_count += 1\n",
    "\n",
    "            for j in range(PSPACE_LEN):\n",
    "\n",
    "                RM_ij=np.array(RHO_MOD[i][0][j])\n",
    "                FG_ij=RHO_MOD[i][1][j]\n",
    "                N_BG_ij=len(RM_ij[ RM_ij > FG_ij])\n",
    "\n",
    "                # False Alarm probability\n",
    "                falses_ij = len(RM_ij[RM_ij > thrshld])\n",
    "                False_count += falses_ij\n",
    "\n",
    "        # Detection/False Alarm probability stats\n",
    "        Detect_stat = Detect_count / trials\n",
    "        False_stat = False_count / (len(RHO_MOD[i][0][0]) * PSPACE_LEN * trials)\n",
    "\n",
    "        # Appending stat per threshold\n",
    "        Detection_Prob.append(Detect_stat)\n",
    "        New_False_Prob.append(False_stat)\n",
    "        \n",
    "        return(Detection_Prob, New_False_Prob)\n",
    "\n",
    "def ROC_Curve(n_s, N, new_stat_only=True, outputfile=\"ROC_test\"):\n",
    "\n",
    "    with open(\"Merged_Peak.json\", \"r\") as f:\n",
    "        RHO_MOD = json.load(f)\n",
    "\n",
    "    with open(\"Merged_thresholds.json\", \"r\") as f:\n",
    "        thresholds = json.load(f)\n",
    "    \n",
    "    # Choose soley n-rho's or an addition stat\n",
    "    if new_stat_only == True:\n",
    "        stat_list = []\n",
    "    else:\n",
    "        stat_list = [\"CC_IJ\",\"CS_IJ\"]\n",
    "\n",
    "    # different ^n rhos are appended\n",
    "    for n in range(1,n_s+1):\n",
    "        rho = \"CC_IJ/((1+abs(CS_IJ))**{})\".format(n)\n",
    "        stat_list.append(rho)\n",
    "\n",
    "    stat_length = len(stat_list)\n",
    "    for s in range(stat_length):\n",
    "\n",
    "        rho(stat = stat_list[s])\n",
    "        window(.02)\n",
    "        Detection_Prob, New_False_Prob = ROC_Data(min(thresholds), max(thresholds), N)\n",
    "        plt.plot(New_False_Prob, Detection_Prob, label=stat_list[s])\n",
    "\n",
    "    plt.xlabel(\"New_False_Probs\")\n",
    "    plt.ylabel(\"Detection_Probs\")\n",
    "    plt.title(\"ROC Curve:n's={}:N={}\".format(n_s,N))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"{}-ROC.png\".format(outputfile))\n",
    "    plt.show()\n",
    "\n",
    "def Scatter_plotter(thrshld, xvar, yvar): \n",
    "\n",
    "    with open(\"Merged_essentials.json\", \"r\") as f:\n",
    "        essentials = json.load(f)\n",
    "    A_LEN = essentials[\"essentials\"][0]\n",
    "    F_LEN = essentials[\"essentials\"][1]\n",
    "    G_LEN = essentials[\"essentials\"][2]\n",
    "    F_RANGE = essentials[\"essentials\"][3]\n",
    "    A_RANGE = essentials[\"essentials\"][4]\n",
    "    G_RANGE = essentials[\"essentials\"][5]\n",
    "    AFG_PAIR = essentials[\"essentials\"][6]\n",
    "    trials = essentials[\"essentials\"][7]\n",
    "\n",
    "    with open(\"Merged_output.json\", \"r\") as f:\n",
    "        output = json.load(f)\n",
    "\n",
    "    label=[\"amplitude\",\"frequency\", \"gamma\"]\n",
    "    PSPACE_LEN=len(AFG_PAIR)\n",
    "    SUCC_PAIRS=([],[])\n",
    "    FAIL_PAIRS=([],[])\n",
    "\n",
    "    #loops from trial/ parameter space pairs \n",
    "    for i in range(trials):\n",
    "\n",
    "        succ_count_thrhld=0  #test condition for adding to fail array\n",
    "\n",
    "        for j in range(PSPACE_LEN):\n",
    "\n",
    "            RM_ij=np.array(RHO_MOD[i][0][j])  #get moded rho_ij background values\n",
    "            FG_ij=RHO_MOD[i][1][j]            #get moded rho_ij foreground value\n",
    "            N_BG_ij=len(RM_ij[ RM_ij > FG_ij])     # get background vals > froeground vals\n",
    "\n",
    "            #tests if the given pair passes the threshold test\n",
    "            if ((N_BG_ij==0) and (FG_ij > thrshld)):\n",
    "\n",
    "                parameter_x=output[i][0][xvar]  #get parameter 'xvar' of trial i\n",
    "                parameter_y=output[i][0][yvar]  #get parameter 'yvar' of trial i\n",
    "                SUCC_PAIRS[0].append(parameter_x) #add to successes\n",
    "                SUCC_PAIRS[1].append(parameter_y)\n",
    "                succ_count_thrhld+=1\n",
    "                break\n",
    "\n",
    "        #sees if trial i failed the test; adds parameters to fail array\n",
    "        if succ_count_thrhld==0:\n",
    "            parameter_x=output[i][0][xvar]\n",
    "            parameter_y=output[i][0][yvar]\n",
    "            FAIL_PAIRS[0].append(parameter_x)\n",
    "            FAIL_PAIRS[1].append(parameter_y)\n",
    "\n",
    "    colors = (\"red\", \"blue\")\n",
    "    groups = (\"pass\", \"fail\")\n",
    "    marks = (\"o\", \"*\")\n",
    "    data = ( SUCC_PAIRS, FAIL_PAIRS )\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    for data, marks, color, group in zip(data, marks, colors, groups):\n",
    "        x , y = data\n",
    "        ax.scatter(x, y, marker=marks, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "    plt.title('found-missed plot')\n",
    "    plt.xlabel(label[xvar])\n",
    "    plt.ylabel(label[yvar])\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "\n",
    "def index_counter(var1,var2):\n",
    "    \n",
    "    # Calculates the index of the attribute variable\n",
    "    # (A, F, or G) which the heat map should marginaliz \n",
    "    # over\n",
    "    if ((var1 <= 2) and (var2 <= 2)):    #checks if range valid\n",
    "        if ((0<= var1) and (0<= var2)):  #checks if range valid\n",
    "            if (var2 < var1):            #properly orders\n",
    "                var1, var2 = var2, var1\n",
    "                index=(1/4)*(var1-1)*(var1-2)*(var2)*(7-3*var2) #maps to needed index\n",
    "\n",
    "            elif (var2==var1):\n",
    "                index=var2  # if the values are the same, set as index\n",
    "\n",
    "            else:\n",
    "                index=(1/4)*(var1-1)*(var1-2)*(var2)*(7-3*var2) #maps to needed index\n",
    "\n",
    "        else:\n",
    "            print(\"improper index inputs: both must be >= 0\")\n",
    "            index=0\n",
    "    else:\n",
    "        print(\"improper index inputs: both must be <= 2\")\n",
    "        index=0\n",
    "\n",
    "    return index\n",
    "\n",
    "def heatmap(var1, var2, T, max_OS=-1, max_BG=-1):\n",
    "\n",
    "    with open(\"Merged_essentials.json\", \"r\") as f:\n",
    "        essentials = json.load(f)\n",
    "    A_LEN = essentials[\"essentials\"][0]\n",
    "    F_LEN = essentials[\"essentials\"][1]\n",
    "    G_LEN = essentials[\"essentials\"][2]\n",
    "    F_RANGE = essentials[\"essentials\"][3]\n",
    "    A_RANGE = essentials[\"essentials\"][4]\n",
    "    G_RANGE = essentials[\"essentials\"][5]\n",
    "    AFG_PAIR = essentials[\"essentials\"][6]\n",
    "    trials = essentials[\"essentials\"][7]\n",
    "\n",
    "    with open(\"Merged_output.json\", \"r\") as f:\n",
    "        output = json.load(f)\n",
    "    \n",
    "    with open(\"Merged_Max_OS.json\", \"r\") as f:\n",
    "        Max_OS = json.load(f)\n",
    "\n",
    "    with open(\"Merged_Max_BG_TEMP.json\", \"r\") as f:\n",
    "        Max_BG_TEMP = json.load(f)\n",
    "    \n",
    "    index=index_counter(var1,var2)\n",
    "\n",
    "    mapping={}   #initalizes mapping for heat map\n",
    "    count_mapping={} #initalies array to count occurances of certain parameters\n",
    "    BG_mapping={}\n",
    "\n",
    "    holder=np.full((A_LEN, F_LEN, G_LEN), 0, dtype=list)\n",
    "    count_holder=np.full((A_LEN, F_LEN, G_LEN), 0, dtype=list)\n",
    "    BG_holder=np.full((A_LEN, F_LEN, G_LEN), 0, dtype=list)\n",
    "\n",
    "    # builds link between keys and np-array \n",
    "    for j in range(F_LEN):\n",
    "        for i in range(A_LEN):\n",
    "            for k in range(G_LEN):\n",
    "                F=F_RANGE[j]\n",
    "                A=A_RANGE[i]\n",
    "                G=G_RANGE[k]\n",
    "\n",
    "                holder[(i,j,k)]=[0]\n",
    "                count_holder[(i,j,k)]=[0]\n",
    "                BG_holder[(i,j,k)]=[0]\n",
    "\n",
    "                mapping.update({ (A,F,G) : holder[(i,j,k)]})\n",
    "                count_mapping.update({ (A,F,G) : count_holder[(i,j,k)]})\n",
    "                BG_mapping.update({ (A,F,G) : BG_holder[(i,j,k)]})\n",
    "\n",
    "    #form BG mappings\n",
    "    num_j=0\n",
    "    for j in AFG_PAIR:\n",
    "        BG_mapping[tuple(j)][0]=MAX_BG_TEMP[num_j]\n",
    "        num_j+=1\n",
    "\n",
    "    # form OS mappings\n",
    "    for OS in MAX_OS.values():\n",
    "        if (T<=OS[1]):\n",
    "            mapping[OS[0]][0]+=float(OS[1])\n",
    "            count_mapping[OS[0]][0]+=1\n",
    "\n",
    "    #redefines things to be normal arrays inside \n",
    "    for j in range(F_LEN):\n",
    "        for i in range(A_LEN):\n",
    "            for k in range(G_LEN):\n",
    "                holder[(i,j,k)]=holder[(i,j,k)][0]\n",
    "                count_holder[(i,j,k)]=count_holder[(i,j,k)][0]\n",
    "                BG_holder[(i,j,k)]=BG_holder[(i,j,k)][0]\n",
    "\n",
    "    #marginalizes array in direction of index\n",
    "    index=int(index)\n",
    "    plot_array=holder.sum(index)\n",
    "    normalizer=count_holder.sum(index)\n",
    "    BG_plot=BG_holder.sum(index)\n",
    "\n",
    "    #builds a copy of the original array, but fixes things to be integers \n",
    "    pa_cop=np.full(plot_array.shape,0.0)\n",
    "    n_cop=np.full(normalizer.shape,0.0)\n",
    "    BG_cop=np.full(BG_plot.shape,0.0)\n",
    "    z=[(i,j) for i in range(plot_array.shape[0]) for j in range(plot_array.shape[1])]\n",
    "    for tup in z:\n",
    "        pa_cop[tup]=plot_array[tup]\n",
    "        n_cop[tup]=normalizer[tup]\n",
    "        BG_cop[tup]=BG_plot[tup]\n",
    "\n",
    "    #normalize\n",
    "    pa_cop[n_cop>0]=pa_cop[n_cop>0]/n_cop[n_cop>0]\n",
    "\n",
    "    if (max_OS==-1):\n",
    "        vmax_val=pa_cop.max()\n",
    "    else:\n",
    "        vmax_val=max_OS\n",
    "\n",
    "    label=np.array([\"A\", \"F\", \"G\"])\n",
    "    IL=np.array([0, 1, 2])\n",
    "    IL=IL[ IL != index]\n",
    "\n",
    "    XV=\"self.\"+label[IL[1]]+\"_RANGE\"\n",
    "    XA=eval(XV)\n",
    "\n",
    "    YV=\"self.\"+label[IL[0]]+\"_RANGE\"\n",
    "    YA=eval(YV)\n",
    "\n",
    "    contours = plt.contour(XA, YA, pa_cop, 10, colors='blue')\n",
    "    plt.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "    plt.contourf(XA, YA, pa_cop, 100, cmap='hot', alpha=1, vmin=T, vmax=vmax_val);\n",
    "    plt.colorbar();\n",
    "\n",
    "    plt.xlabel(label[IL[1]])\n",
    "    plt.ylabel(label[IL[0]])\n",
    "    plt.title('Max On Source; t='+str(T))\n",
    "    plt.show()\n",
    "\n",
    "    if (max_BG==-1):\n",
    "        vmax_val=BG_cop.max()\n",
    "    else:\n",
    "        vmax_val=max_BG\n",
    "\n",
    "    contours = plt.contour(XA, YA, BG_cop, 10, colors='blue')\n",
    "    plt.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "    plt.contourf(XA, YA, BG_cop, 100, cmap='hot', alpha=1, vmin=T, vmax=vmax_val);\n",
    "    plt.colorbar();\n",
    "\n",
    "    plt.xlabel(label[IL[1]])\n",
    "    plt.ylabel(label[IL[0]])\n",
    "    plt.title('Max Background; t='+str(T))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447858fc",
   "metadata": {},
   "source": [
    "# JSON Options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a96aab",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Merged_essentials.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-632d1d5a1abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# All jsons to open (post-combining)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Merged_essentials.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0messentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mA_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0messentials\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"essentials\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Merged_essentials.json'"
     ]
    }
   ],
   "source": [
    "# All jsons to open (post-combining)\n",
    "\n",
    "with open(\"Merged_essentials.json\", \"r\") as f:\n",
    "    essentials = json.load(f)\n",
    "A_LEN = essentials[\"essentials\"][0]\n",
    "F_LEN = essentials[\"essentials\"][1]\n",
    "G_LEN = essentials[\"essentials\"][2]\n",
    "F_RANGE = essentials[\"essentials\"][3]\n",
    "A_RANGE = essentials[\"essentials\"][4]\n",
    "G_RANGE = essentials[\"essentials\"][5]\n",
    "AFG_PAIR = essentials[\"essentials\"][6]\n",
    "trials = essentials[\"essentials\"][7]\n",
    "\n",
    "with open(\"Merged_output.json\", \"r\") as f:\n",
    "    output = json.load(f)\n",
    "\n",
    "with open(\"Merged_Peak.json\", \"r\") as f:\n",
    "    RHO_MOD = json.load(f)\n",
    "\n",
    "with open(\"Merged_Max_OS.json\", \"r\") as f:\n",
    "    Max_OS = json.load(f)\n",
    "\n",
    "with open(\"Merged_Max_BG_TEMP.json\", \"r\") as f:\n",
    "    Max_BG_TEMP = json.load(f)\n",
    "\n",
    "with open(\"Merged_thresholds.json\", \"r\") as f:\n",
    "    thresholds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ecbaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
